{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8ed292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "try:\n",
    "    import dill as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082c1a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=52\n",
      "Photos: train=52\n"
     ]
    }
   ],
   "source": [
    "# Loading Image Features\n",
    "train_features = load(open(\"encoded_train_images.pickle\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "# Load Train Descriptions, which contains, 5 captions corresponding to a key\n",
    "train_descriptions = load(open(\"train_descriptions.pickle\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_descriptions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ffd91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 247 -> 3\n",
      "Description Length: 9\n"
     ]
    }
   ],
   "source": [
    "train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        train_captions.append(cap)\n",
    "len(train_captions)\n",
    "\n",
    "threshold = 7\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        if w not in word_counts:\n",
    "            word_counts[w] = 0\n",
    "        word_counts[w] += 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= threshold]\n",
    "print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
    "vocab_size\n",
    "\n",
    "\n",
    "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
    "vocab_size\n",
    "\n",
    "def max_length(descriptions): \n",
    "    max_l = -1e9\n",
    "    for key, cap in descriptions.items():\n",
    "        for line in cap:\n",
    "            max_l = max(max_l, len(line.split(' ')))\n",
    "    return max_l\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "132c4626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-e28b105c32ea>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm(f):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6a49d2b89f4a00b26b9e649489e9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-e28b105c32ea>:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for word, i in tqdm(wordtoix.items()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50e2e961af4046b806f9fd62a429be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "glove_dir = ''\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6b.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(wordtoix.items()):\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "embedding_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b609cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Embedding_Matrix.pkl', 'wb') as embedding_pickle:\n",
    "    pickle.dump(embedding_matrix, embedding_pickle)\n",
    "    \n",
    "with open('wordtoix.pkl', 'wb') as wti_pickle:\n",
    "    pickle.dump(wordtoix, wti_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098fbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
